<div align="center">

# DATASCRIBE

**Universal PDF to CSV Extractor**

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE.md)
[![Python](https://img.shields.io/badge/Python-3.6+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![Build Status](https://img.shields.io/badge/Build-Passing-brightgreen.svg)](https://github.com/FoldedOdin/DataScribe)
[![Version](https://img.shields.io/badge/Version-1.0.0-blue.svg)](https://github.com/FoldedOdin/DataScribe)

**Author:** Karthik K Pradeep

</div>

## ğŸŒŸ About

A **Python script designed to automatically extract tabular data** from multiple PDF files and consolidate it into a single, clean CSV file. Built to be robust, handling various PDF layouts, messy tables, and even PDFs that don't contain structured tables by falling back to raw text extraction.

**Perfect for data science and analysis projects where source data is locked away in PDF reports.**

## âœ¨ Features

### ğŸ“¦ Bulk Processing
- **ğŸ”„ Batch Extraction** - Process all PDFs in a directory at once
- **âš¡ Automated Pipeline** - Set it and forget it processing
- **ğŸ“ Directory Scanning** - Automatically finds all PDF files

### ğŸ§  Intelligent Extraction
- **ğŸ¯ Smart Table Detection** - Uses camelot-py for accurate parsing
- **ğŸ”€ Dual Method Approach** - Tries both 'lattice' and 'stream' flavors
- **ğŸ“Š Layout Agnostic** - Handles various PDF table formats

### ğŸ›¡ï¸ Fallback Mechanism
- **ğŸ“ Text Extraction** - Falls back to PyPDF2 when tables aren't found
- **ğŸ”§ Structure Attempt** - Tries to organize raw text into columns
- **âœ… Never Fails Empty** - Always extracts something useful

### ğŸ§¹ Data Cleaning
- **ğŸ”¢ Type Conversion** - Automatically converts to numeric types
- **ğŸ—‘ï¸ Handles Noise** - Manages non-numeric values gracefully
- **ğŸ“‹ Column Filtering** - Keep only the columns you need
- **ğŸ¯ Standardization** - Cleans and normalizes column names

### ğŸ“… Temporal Context
- **ğŸ—“ï¸ Year Extraction** - Automatically extracts year from filenames
- **ğŸ“Š Time Series Ready** - Adds temporal dimension to your data
- **ğŸ” Pattern Recognition** - Finds four-digit years in any filename format

### ğŸ” Robust Logging
- **ğŸ“ Error Tracking** - Logs all errors with timestamps
- **ğŸ› Easy Debugging** - Identifies problematic files quickly
- **ğŸ“Š Processing Summary** - Shows success/failure statistics

### ğŸ“‚ Organized Output
- **ğŸ’¾ Intermediate Files** - Saves CSV for each extracted table
- **ğŸ“Š Merged Dataset** - Combines all data into single file
- **ğŸ—‚ï¸ Clean Structure** - Organized temporary directory

## ğŸ› ï¸ Tech Stack

<div align="center">

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)

**Core Libraries:**

![Camelot](https://img.shields.io/badge/Camelot-PDF%20Extraction-orange?logo=python&logoColor=white)
![PyPDF2](https://img.shields.io/badge/PyPDF2-Text%20Extraction-blue?logo=python&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-5C3EE8?logo=opencv&logoColor=white)

</div>

## ğŸ”„ How It Works

DataScribe follows a sophisticated multi-step pipeline:

### ğŸ“Š Processing Pipeline

```
1. ğŸ” Scan PDFs â†’ 2. ğŸ“„ Extract Tables â†’ 3. ğŸ§¹ Clean Data â†’ 4. ğŸ’¾ Save CSVs â†’ 5. ğŸ”— Merge All
```

### Detailed Workflow

**Step 1: PDF Discovery**
- Scans `INPUT_DIR` for all `.pdf` files
- Creates list of files to process

**Step 2: Data Extraction**
For each PDF file:
- ğŸ“… **Extract Year**: Parses filename for four-digit year (e.g., 2021)
- ğŸ“Š **Extract Tables**: Uses camelot with both 'lattice' and 'stream' methods
- ğŸ“ **Fallback to Text**: If camelot fails, uses PyPDF2 for raw text extraction

**Step 3: Data Processing**
Each extracted table undergoes:
- ğŸ§¹ Column cleaning and standardization
- ğŸ”¢ Numeric type conversion
- ğŸ¯ Filtering to retain only `TARGET_COLS`
- ğŸ“… Addition of extracted 'Year' column

**Step 4: Intermediate Storage**
- ğŸ’¾ Each cleaned table saved as separate CSV
- ğŸ“ Stored in `temp_csvs` directory
- ğŸ·ï¸ Named with source PDF and table index

**Step 5: Final Consolidation**
- ğŸ”— Concatenates all temporary CSVs
- ğŸ“Š Creates master file: `merged_dataset.csv`
- ğŸ“ Logs any errors to `extraction_log.txt`

## ğŸš€ Getting Started

### ğŸ“‹ Prerequisites

Before you begin, ensure you have:

- ![Python](https://img.shields.io/badge/Python-3.6+-3776AB?logo=python&logoColor=white) (v3.6 or higher)
- ![Ghostscript](https://img.shields.io/badge/Ghostscript-Required-purple.svg) (system dependency)

### âš¡ Installation

#### Step 1: Install System Dependencies (Ghostscript)

Camelot requires Ghostscript to be installed on your system.

<details>
<summary><b>ğŸªŸ Windows Installation</b></summary>

1. Download the installer from [Ghostscript website](https://www.ghostscript.com/download/gsdnld.html)
2. Run the installer and follow the setup wizard
3. Verify installation:
```bash
gs --version
```

</details>

<details>
<summary><b>ğŸ macOS Installation (Homebrew)</b></summary>

```bash
# Install Homebrew if needed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install Ghostscript
brew install ghostscript

# Verify installation
gs --version
```

</details>

<details>
<summary><b>ğŸ§ Linux Installation (Debian/Ubuntu)</b></summary>

```bash
# Update package list
sudo apt-get update

# Install Ghostscript
sudo apt-get install ghostscript

# Verify installation
gs --version
```

</details>

#### Step 2: Install Python Libraries

Install all required Python packages:

```bash
pip install pandas "camelot-py[cv]" PyPDF2
```

Or use requirements.txt:

```bash
pip install -r requirements.txt
```

### âš™ï¸ Configuration

Open `PDFtoCSV.py` and modify the CONFIG section:

```python
# ===== CONFIG =====
INPUT_DIR = "path/to/your/pdf/folder"  # Your PDF directory
OUTPUT_FILE = "merged_dataset.csv"      # Final output filename
TEMP_DIR = "temp_csvs"                  # Temporary CSV storage
LOG_FILE = "extraction_log.txt"         # Error log file

# Optional: Specify columns to keep (leave empty for all)
TARGET_COLS = ["Turbidity", "Temperature", "pH", "TDS"]
```

### ğŸ“– Usage

1ï¸âƒ£ **Prepare your PDFs**: Place all PDF files in your `INPUT_DIR`

2ï¸âƒ£ **Run the script**:

```bash
python PDFtoCSV.py
```

3ï¸âƒ£ **Check results**: Find your consolidated data in `merged_dataset.csv`

### Example Output

```bash
Processing: report_2021.pdf
  âœ“ Found 2 tables
  âœ“ Extracted year: 2021
  âœ“ Saved: report_2021_2021_0.csv
  âœ“ Saved: report_2021_2021_1.csv

Processing: data_2022.pdf
  âœ“ Found 1 table
  âœ“ Extracted year: 2022
  âœ“ Saved: data_2022_2022_0.csv

=================================
Extraction Complete!
=================================
âœ“ PDFs processed: 2
âœ“ Tables extracted: 3
âœ“ Output file: merged_dataset.csv
âœ— Failed files: 0
ğŸ“ Check extraction_log.txt for details
```

## ğŸ—ï¸ Project Structure

```
ğŸ“¦ DataScribe/
â”œâ”€â”€ ğŸ“„ PDFtoCSV.py                 # Main extraction script
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“„ extraction_log.txt          # Error log (generated)
â”œâ”€â”€ ğŸ“Š merged_dataset.csv          # Final output (generated)
â”œâ”€â”€ ğŸ“ your_pdf_folder/            # Input PDFs
â”‚   â”œâ”€â”€ ğŸ“„ report_2021.pdf
â”‚   â”œâ”€â”€ ğŸ“„ data_2022.pdf
â”‚   â””â”€â”€ ğŸ“„ results_2023.pdf
â”œâ”€â”€ ğŸ“ temp_csvs/                  # Intermediate CSVs (generated)
â”‚   â”œâ”€â”€ ğŸ“„ report_2021_2021_0.csv
â”‚   â”œâ”€â”€ ğŸ“„ report_2021_2021_1.csv
â”‚   â”œâ”€â”€ ğŸ“„ data_2022_2022_0.csv
â”‚   â””â”€â”€ ğŸ“„ results_2023_2023_0.csv
â”œâ”€â”€ ğŸ“„ LICENSE                     # MIT License
â””â”€â”€ ğŸ“„ README.md                   # You are here
```

## âš™ï¸ Advanced Configuration

### Custom Column Mapping

```python
# Map PDF column names to standard names
COLUMN_MAPPING = {
    "Temp (Â°C)": "Temperature",
    "Turb (NTU)": "Turbidity",
    "pH Level": "pH"
}
```

### Filtering Options

```python
# Keep specific columns
TARGET_COLS = ["Temperature", "pH", "TDS"]

# Or keep all columns (empty list)
TARGET_COLS = []
```

### Extraction Methods

```python
# Prioritize lattice method (for bordered tables)
METHODS = ['lattice', 'stream']

# Or try stream first (for borderless tables)
METHODS = ['stream', 'lattice']
```

## ğŸ”§ Troubleshooting

### Common Issues

<details>
<summary><b>âŒ "Ghostscript not found" error</b></summary>

**Solution**: Make sure Ghostscript is installed and accessible in your system PATH.

**Verify installation**:
```bash
gs --version
```

If not found, reinstall Ghostscript and restart your terminal.

</details>

<details>
<summary><b>âŒ "No module named 'camelot'" error</b></summary>

**Solution**: Install camelot with CV support:
```bash
pip install "camelot-py[cv]"
```

</details>

<details>
<summary><b>âš ï¸ No tables extracted from PDF</b></summary>

**Possible causes**:
- PDF contains only images (scanned documents)
- Tables have complex layouts
- PDF is password-protected

**Solutions**:
- Use OCR for scanned PDFs (pytesseract)
- Try adjusting camelot flavor settings
- Remove password protection first

</details>

<details>
<summary><b>âš ï¸ Incorrect data extraction</b></summary>

**Solution**: Check `temp_csvs` folder to see intermediate results. Adjust:
- Column filtering in `TARGET_COLS`
- Extraction method priority
- Data cleaning rules

</details>

<details>
<summary><b>ğŸŒ Slow processing speed</b></summary>

**Solution**: Processing speed depends on:
- PDF complexity and size
- Number of tables per PDF
- System resources

Consider processing in batches for large datasets.

</details>

## ğŸ“Š Example Use Cases

### Research Data Collection
```bash
# Extract water quality data from monthly reports
TARGET_COLS = ["Date", "pH", "Temperature", "Turbidity", "DO"]
INPUT_DIR = "water_quality_reports/"
```

### Financial Data Analysis
```bash
# Extract quarterly financial metrics
TARGET_COLS = ["Quarter", "Revenue", "Expenses", "Profit"]
INPUT_DIR = "financial_reports/"
```

### Scientific Data Mining
```bash
# Extract experimental results from publications
TARGET_COLS = ["Sample", "Measurement", "Result", "Error"]
INPUT_DIR = "research_papers/"
```

## ğŸ§ª Testing

Test with sample PDFs first:

```bash
# Create test directory
mkdir test_pdfs
cp sample_*.pdf test_pdfs/

# Update config
INPUT_DIR = "test_pdfs"

# Run script
python PDFtoCSV.py

# Verify output
head merged_dataset.csv
```

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!

<div align="center">

[![Issues](https://img.shields.io/github/issues/yourusername/datascribe)](https://github.com/yourusername/datascribe/issues)
[![Pull Requests](https://img.shields.io/github/issues-pr/yourusername/datascribe)](https://github.com/yourusername/datascribe/pulls)

</div>

### How to Contribute

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add amazing feature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/amazing-feature`)
5. ğŸ“¥ Open a Pull Request

## ğŸ“ Roadmap

- [ ] Support for scanned PDFs (OCR integration)
- [ ] GUI interface for easier configuration
- [ ] Multi-language support for international documents
- [ ] Cloud storage integration (Google Drive, Dropbox)
- [ ] Real-time progress bar during extraction
- [ ] Support for Excel and Word documents
- [ ] Automated data validation and quality checks
- [ ] Docker container for easy deployment

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE.md) file for details.

<div align="center">

[![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

## ğŸ™ Acknowledgments

<div align="center">

Special thanks to these amazing projects:

[![Python](https://img.shields.io/badge/Thanks-Python%20Community-3776AB?logo=python&logoColor=white)](https://python.org)
[![Pandas](https://img.shields.io/badge/Thanks-Pandas%20Team-150458?logo=pandas&logoColor=white)](https://pandas.pydata.org)
[![Camelot](https://img.shields.io/badge/Thanks-Camelot%20Developers-orange?logo=python&logoColor=white)](https://camelot-py.readthedocs.io)

</div>

## ğŸ“¬ Connect With Author

<div align="center">

**Karthik K Pradeep**

[![GitHub](https://img.shields.io/badge/GitHub-Follow-181717?logo=github&logoColor=white)](https://github.com/FoldedOdin)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?logo=linkedin&logoColor=white)](https://linkedin.com/in/karthikkpradeep)
[![Email](https://img.shields.io/badge/Email-Contact-D14836?logo=gmail&logoColor=white)](mailto:karthikkpradeep123@gmail.com)

</div>

## ğŸ†˜ Support

Need help with DataScribe?

- ğŸ’¬ **Issues**: [Open an issue](https://github.com/FoldedOdin/DataScribe/issues)
- ğŸ“– **Wiki**: [Documentation](https://github.com/FoldedOdin/DataScribe/wiki)

---

<div align="center">

**ğŸ¯ Built with â¤ï¸ for data scientists and analysts**

![Made with Love](https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg)
![Open Source](https://img.shields.io/badge/Open%20Source-ğŸ’™-blue.svg)
![Python](https://img.shields.io/badge/Python-ğŸ’›-yellow.svg)

### â­ If you found this project helpful, please give it a star!

[![ğŸš€ Return To TOP](https://img.shields.io/badge/ğŸš€%20Return%20to-%20TOP-FF6B6B?style=for-the-badge&labelColor=4ECDC4)](#datascribe)

</div>
